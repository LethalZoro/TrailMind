{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prediction: 5.109289646148682\n",
      "Predicted class: Mint\n",
      "Confidence: 35.10%\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"dima806/medicinal_plants_image_detection\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"dima806/medicinal_plants_image_detection\")\n",
    "model\n",
    "\n",
    "# Load the local image file\n",
    "image = Image.open(\"download (2).jpeg\")\n",
    "\n",
    "# Process the image for model input\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "time_start = time.time()\n",
    "# Get model predictions\n",
    "outputs = model(**inputs)\n",
    "time_end = time.time()\n",
    "print(\"Time taken for prediction:\", time_end - time_start)\n",
    "predictions = outputs.logits.softmax(dim=1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class_idx = predictions.argmax().item()\n",
    "predicted_class = model.config.id2label[predicted_class_idx]\n",
    "confidence = predictions[0][predicted_class_idx].item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")\n",
    "\n",
    "# 0.03592205047607422 laptop gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (key_monitor):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_2321/966556466.py\", line 40, in key_monitor\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/keyboard/__init__.py\", line 410, in is_pressed\n",
      "    _listener.start_if_necessary()\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/keyboard/_generic.py\", line 35, in start_if_necessary\n",
      "    self.init()\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/keyboard/__init__.py\", line 196, in init\n",
      "    _os_keyboard.init()\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/keyboard/_nixkeyboard.py\", line 113, in init\n",
      "    build_device()\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/keyboard/_nixkeyboard.py\", line 109, in build_device\n",
      "    ensure_root()\n",
      "  File \"/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/keyboard/_nixcommon.py\", line 174, in ensure_root\n",
      "    raise ImportError('You must be root to use this library on linux.')\n",
      "ImportError: You must be root to use this library on linux.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press and hold SPACE to record. Release to transcribe. Press ESC to quit.\n",
      "Interrupted by user\n",
      "Transcription stopped\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pynput import keyboard as pynput_keyboard\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# Setup audio parameters\n",
    "sample_rate = 16000  # Sample rate expected by Whisper\n",
    "channels = 1\n",
    "dtype = 'float32'\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# Create a queue to store audio chunks\n",
    "audio_queue = queue.Queue()\n",
    "is_recording = False\n",
    "stop_recording = False\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"This is called for each audio block\"\"\"\n",
    "    if is_recording:\n",
    "        # Add the current chunk to our queue\n",
    "        audio_queue.put(indata.copy())\n",
    "\n",
    "def key_monitor():\n",
    "    \"\"\"Monitor space bar presses and control recording\"\"\"\n",
    "    global is_recording, stop_recording\n",
    "    \n",
    "    print(\"Press and hold SPACE to record. Release to transcribe. Press ESC to quit.\")\n",
    "    \n",
    "    while not stop_recording:\n",
    "        # Start recording when space is pressed\n",
    "        if keyboard.is_pressed('space') and not is_recording:\n",
    "            is_recording = True\n",
    "            audio_queue.queue.clear()  # Clear any old audio\n",
    "            print(\"Recording... (holding space)\")\n",
    "        \n",
    "        # Stop recording when space is released\n",
    "        elif not keyboard.is_pressed('space') and is_recording:\n",
    "            is_recording = False\n",
    "            print(\"Processing...\")\n",
    "            process_audio()\n",
    "        \n",
    "        # Exit on ESC\n",
    "        if keyboard.is_pressed('esc'):\n",
    "            stop_recording = True\n",
    "            is_recording = False\n",
    "            print(\"Stopping...\")\n",
    "        \n",
    "        time.sleep(0.01)  # Small sleep to prevent CPU hogging\n",
    "\n",
    "def process_audio():\n",
    "    \"\"\"Process collected audio chunks and transcribe\"\"\"\n",
    "    if audio_queue.empty():\n",
    "        print(\"No audio recorded\")\n",
    "        return\n",
    "    \n",
    "    # Combine all audio chunks\n",
    "    chunks = []\n",
    "    while not audio_queue.empty():\n",
    "        chunks.append(audio_queue.get())\n",
    "    \n",
    "    if not chunks:\n",
    "        return\n",
    "        \n",
    "    audio_data = np.concatenate(chunks, axis=0)\n",
    "    audio_flat = audio_data.flatten()\n",
    "    \n",
    "    # Process through Whisper\n",
    "    input_features = processor(\n",
    "        audio_flat,\n",
    "        sampling_rate=sample_rate,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # Generate transcription\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"Transcription: {transcription}\")\n",
    "\n",
    "def start_space_bar_transcription():\n",
    "    \"\"\"Start the main transcription loop\"\"\"\n",
    "    global stop_recording, is_recording\n",
    "    \n",
    "    # Reset flags\n",
    "    stop_recording = False\n",
    "    is_recording = False\n",
    "    \n",
    "    # Start audio stream\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=channels, dtype=dtype, callback=audio_callback):\n",
    "        # Start key monitoring in a separate thread\n",
    "        monitor_thread = threading.Thread(target=key_monitor)\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        try:\n",
    "            # Wait for the monitor thread to finish\n",
    "            while not stop_recording:\n",
    "                time.sleep(0.1)\n",
    "        except KeyboardInterrupt:\n",
    "            stop_recording = True\n",
    "            is_recording = False\n",
    "            print(\"Interrupted by user\")\n",
    "        \n",
    "        # Wait for the monitor thread to finish\n",
    "        monitor_thread.join()\n",
    "    \n",
    "    print(\"Transcription stopped\")                     \n",
    "\n",
    "\n",
    "start_space_bar_transcription()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice for linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zoro/.virtualenvs/pi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press and hold SPACE to record. Release to transcribe. Press ESC to quit.\n",
      "Recording... (holding space)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  you\n",
      "Recording... (holding space)\n",
      "Processing...\n",
      "Transcription:  you\n",
      "Recording... (holding space)\n",
      "Processing...\n",
      "Transcription:  Hello, can you hear me now?\n",
      "Recording... (holding space)\n",
      "Processing...\n",
      "Transcription:  It looks like the problem was in the mic and it was not connected.\n",
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pynput import keyboard as pynput_keyboard\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# Setup audio parameters\n",
    "sample_rate = 16000  # Sample rate expected by Whisper\n",
    "channels = 1\n",
    "dtype = 'float32'\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# Create a queue to store audio chunks\n",
    "audio_queue = queue.Queue()\n",
    "is_recording = False\n",
    "stop_recording = False\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"This is called for each audio block\"\"\"\n",
    "    if is_recording:\n",
    "        audio_queue.put(indata.copy())\n",
    "        # print(f\"Captured audio chunk of shape {indata.shape}\")\n",
    "\n",
    "def key_monitor():\n",
    "    \"\"\"Monitor space bar and ESC using pynput\"\"\"\n",
    "    global is_recording, stop_recording\n",
    "\n",
    "    def on_press(key):\n",
    "        global is_recording\n",
    "        try:\n",
    "            # Check if key is the space key using pynput's Key.space\n",
    "            if key == pynput_keyboard.Key.space and not is_recording:\n",
    "                is_recording = True\n",
    "                audio_queue.queue.clear()\n",
    "                print(\"Recording... (holding space)\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def on_release(key):\n",
    "        global is_recording, stop_recording\n",
    "        try:\n",
    "            if key == pynput_keyboard.Key.space and is_recording:\n",
    "                is_recording = False\n",
    "                print(\"Processing...\")\n",
    "                process_audio()\n",
    "        except AttributeError:\n",
    "            if key == pynput_keyboard.Key.esc:\n",
    "                stop_recording = True\n",
    "                is_recording = False\n",
    "                print(\"Stopping...\")\n",
    "                return False  # Stop listener\n",
    "\n",
    "    print(\"Press and hold SPACE to record. Release to transcribe. Press ESC to quit.\")\n",
    "    with pynput_keyboard.Listener(on_press=on_press, on_release=on_release) as listener:\n",
    "        listener.join()\n",
    "\n",
    "def process_audio():\n",
    "    \"\"\"Process collected audio chunks and transcribe\"\"\"\n",
    "    if audio_queue.empty():\n",
    "        print(\"No audio recorded\")\n",
    "        return\n",
    "    \n",
    "    chunks = []\n",
    "    while not audio_queue.empty():\n",
    "        chunks.append(audio_queue.get())\n",
    "    \n",
    "    if not chunks:\n",
    "        return\n",
    "        \n",
    "    audio_data = np.concatenate(chunks, axis=0)\n",
    "    audio_flat = audio_data.flatten()\n",
    "    \n",
    "    input_features = processor(\n",
    "        audio_flat,\n",
    "        sampling_rate=sample_rate,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(f\"Transcription: {transcription}\")\n",
    "\n",
    "def start_space_bar_transcription():\n",
    "    \"\"\"Start the main transcription loop\"\"\"\n",
    "    global stop_recording, is_recording\n",
    "    \n",
    "    stop_recording = False\n",
    "    is_recording = False\n",
    "    \n",
    "    with sd.InputStream(samplerate=sample_rate, channels=channels, dtype=dtype, callback=audio_callback):\n",
    "        monitor_thread = threading.Thread(target=key_monitor)\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        try:\n",
    "            while not stop_recording:\n",
    "                time.sleep(0.1)\n",
    "        except KeyboardInterrupt:\n",
    "            stop_recording = True\n",
    "            is_recording = False\n",
    "            print(\"Interrupted by user\")\n",
    "        \n",
    "        monitor_thread.join()\n",
    "    \n",
    "    print(\"Transcription stopped\")                     \n",
    "\n",
    "start_space_bar_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import time\n",
    "import shlex\n",
    "\n",
    "async def speak(text):\n",
    "    # engine.say(text)\n",
    "    # engine.runAndWait()\n",
    "    # command = f'echo \"{text}\" | ./rasp/piper/piper/piper --model ./piper/en_US-kathleen-low.onnx --rate 125 --output-raw | aplay -r 16050 -f S16_LE -t raw -'\n",
    "    \n",
    "    safe_text = shlex.quote(text)\n",
    "    command = (\n",
    "        f\"echo {safe_text} | piper --model en_US-lessac-medium.onnx --output-raw | \"\n",
    "        f\"aplay -r 22050 -f S16_LE -t raw -\"\n",
    "    )\n",
    "    # Execute the command\n",
    "    text_length = len(text)\n",
    "    duration_per_character = 0.05  # Example: 50 milliseconds per character\n",
    "    estimated_duration = text_length * duration_per_character\n",
    "\n",
    "    subprocess.run(command, shell=True)\n",
    "    await asyncio.sleep(1)\n",
    "    # time.sleep(estimated_duration+4)\n",
    "    print(\"TTS is done\")\n",
    "\n",
    "# asyncio.run(speak(\"Hello how are you doing on a fine day like this?\"))            \n",
    "# speak(\"Hello how are you doing on a fine day like this?\")                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, letâ€™s delve into why light speed is considered constant. "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m         buffer = buffer[end_idx:]  \u001b[38;5;66;03m# Keep the remainder\u001b[39;00m\n\u001b[32m     30\u001b[39m         \u001b[38;5;28mprint\u001b[39m(complete_sentence, end=\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m         \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplete_sentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call your TTS function here\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# If anything is left unspoken after the loop, speak it\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer.strip():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.generate(\n",
    "                model=\"gemma3:1b\",\n",
    "                prompt=f\"\"\"You are an outdoor voice assistant. Explain in detail.\n",
    "                Current query: {\"Explain why light speed is constant\"}\n",
    "                Remember to answer the question without any preamble or introduction.\"\"\",\n",
    "                stream=True,\n",
    "                options={\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"num_predict\": 1000  # Limit response length\n",
    "                }\n",
    "            )\n",
    "\n",
    "import re\n",
    "\n",
    "buffer = \"\"\n",
    "sentence_endings = re.compile(r'([.!?])')  # Detect sentence-ending punctuation\n",
    "\n",
    "for chunk in response:\n",
    "    chunk_text = chunk['response']\n",
    "    buffer += chunk_text\n",
    "    # Check for full sentences\n",
    "    while True:\n",
    "        match = sentence_endings.search(buffer)\n",
    "        if not match:\n",
    "            break  # No sentence-ending punctuation yet\n",
    "        end_idx = match.end()\n",
    "        complete_sentence = buffer[:end_idx].strip()\n",
    "        buffer = buffer[end_idx:]  # Keep the remainder\n",
    "        print(complete_sentence, end=' ', flush=True)\n",
    "        asyncio.run(speak(complete_sentence))  # Call your TTS function here\n",
    "\n",
    "# If anything is left unspoken after the loop, speak it\n",
    "if buffer.strip():\n",
    "    print(buffer.strip(), end=' ', flush=True)\n",
    "    asyncio.run(speak(buffer.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m speak(buffer.strip())\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Run it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_and_speak\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "async def stream_and_speak():\n",
    "    response = ollama.generate(\n",
    "        model=\"gemma3:1b\",\n",
    "        prompt=(\n",
    "            \"You are an outdoor voice assistant. Explain in detail.\\n\"\n",
    "            \"Current query: Explain why light speed is constant.\\n\"\n",
    "            \"Remember to answer the question without any preamble or introduction.\"\n",
    "        ),\n",
    "        stream=True,\n",
    "        options={\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_predict\": 1000\n",
    "        }\n",
    "    )\n",
    "\n",
    "    buffer = \"\"\n",
    "    sentence_endings = re.compile(r'([.!?])')\n",
    "\n",
    "    for chunk in response:\n",
    "        chunk_text = chunk['response']\n",
    "        buffer += chunk_text\n",
    "\n",
    "        while True:\n",
    "            match = sentence_endings.search(buffer)\n",
    "            if not match:\n",
    "                break\n",
    "            end_idx = match.end()\n",
    "            complete_sentence = buffer[:end_idx].strip()\n",
    "            buffer = buffer[end_idx:]\n",
    "            print(complete_sentence, end=' ', flush=True)\n",
    "            await speak(complete_sentence)\n",
    "\n",
    "    if buffer.strip():\n",
    "        print(buffer.strip(), end=' ', flush=True)\n",
    "        await speak(buffer.strip())\n",
    "\n",
    "# Run it\n",
    "asyncio.run(stream_and_speak())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)\n",
    "output[0][0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res=\u001b[43mgenerate_response\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mHello how are you doing. I am very happy today\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[32m      3\u001b[39m speak(res)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_response' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
